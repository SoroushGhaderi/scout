{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98428fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FotMob Field Inspector Initialized ‚úì\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Any, Dict, List, Set\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "FOTMOB_DATA_DIR = Path(\"data/fotmob/matches\")\n",
    "\n",
    "print(\"FotMob Field Inspector Initialized ‚úì\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0392393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FotMob files found: 150\n",
      "\n",
      "Sample files:\n",
      "  - data\\fotmob\\matches\\20251001\\match_1000002056.json.gz\n",
      "  - data\\fotmob\\matches\\20251001\\match_1000003608.json.gz\n",
      "  - data\\fotmob\\matches\\20251001\\match_1000003609.json.gz\n",
      "  - data\\fotmob\\matches\\20251001\\match_4692335.json.gz\n",
      "  - data\\fotmob\\matches\\20251001\\match_4693111.json.gz\n"
     ]
    }
   ],
   "source": [
    "def load_fotmob_json(file_path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Load FotMob JSON file (handles both .json and .gz files).\"\"\"\n",
    "    if file_path.suffix == '.gz':\n",
    "        with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "def get_all_fotmob_files(limit: int = None) -> List[Path]:\n",
    "    \"\"\"Get all FotMob match files.\"\"\"\n",
    "    files = []\n",
    "    for date_dir in sorted(FOTMOB_DATA_DIR.glob(\"*\")):\n",
    "        if date_dir.is_dir():\n",
    "            for file in sorted(date_dir.glob(\"match_*.json\")) + sorted(date_dir.glob(\"match_*.gz\")):\n",
    "                files.append(file)\n",
    "                if limit and len(files) >= limit:\n",
    "                    return files\n",
    "    return files\n",
    "\n",
    "# Load sample files\n",
    "all_files = get_all_fotmob_files()\n",
    "print(f\"Total FotMob files found: {len(all_files)}\")\n",
    "print(f\"\\nSample files:\")\n",
    "for f in all_files[:5]:\n",
    "    print(f\"  - {f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b87f329a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field extraction functions defined ‚úì\n"
     ]
    }
   ],
   "source": [
    "def extract_all_fields(obj: Any, prefix: str = \"\", field_info: Dict = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Recursively extract all fields from a nested JSON structure.\n",
    "    Returns a dictionary with field paths as keys and metadata as values.\n",
    "    \"\"\"\n",
    "    if field_info is None:\n",
    "        field_info = defaultdict(lambda: {\n",
    "            'types': set(),\n",
    "            'sample_values': [],\n",
    "            'null_count': 0,\n",
    "            'total_count': 0,\n",
    "            'is_list': False,\n",
    "            'list_lengths': []\n",
    "        })\n",
    "    \n",
    "    if obj is None:\n",
    "        field_info[prefix]['null_count'] += 1\n",
    "        field_info[prefix]['total_count'] += 1\n",
    "        field_info[prefix]['types'].add('null')\n",
    "        return field_info\n",
    "    \n",
    "    field_info[prefix]['total_count'] += 1\n",
    "    \n",
    "    if isinstance(obj, dict):\n",
    "        field_info[prefix]['types'].add('dict')\n",
    "        for key, value in obj.items():\n",
    "            new_prefix = f\"{prefix}.{key}\" if prefix else key\n",
    "            extract_all_fields(value, new_prefix, field_info)\n",
    "    \n",
    "    elif isinstance(obj, list):\n",
    "        field_info[prefix]['is_list'] = True\n",
    "        field_info[prefix]['list_lengths'].append(len(obj))\n",
    "        field_info[prefix]['types'].add('list')\n",
    "        \n",
    "        # Sample first few items to understand list structure\n",
    "        for i, item in enumerate(obj[:3]):  # Sample first 3 items\n",
    "            new_prefix = f\"{prefix}[{i}]\"\n",
    "            extract_all_fields(item, new_prefix, field_info)\n",
    "    \n",
    "    else:\n",
    "        # Primitive type\n",
    "        field_info[prefix]['types'].add(type(obj).__name__)\n",
    "        if len(field_info[prefix]['sample_values']) < 5:\n",
    "            field_info[prefix]['sample_values'].append(obj)\n",
    "    \n",
    "    return field_info\n",
    "\n",
    "print(\"Field extraction functions defined ‚úì\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bdf85e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing 10 FotMob files...\n",
      "\n",
      "Processing 10/10: match_4727857.json.gz.....\n",
      "\n",
      "‚úì Analysis complete! Found 37399 unique field paths\n"
     ]
    }
   ],
   "source": [
    "# Analyze fields across multiple files\n",
    "num_files_to_analyze = min(10, len(all_files))\n",
    "print(f\"Analyzing {num_files_to_analyze} FotMob files...\\n\")\n",
    "\n",
    "all_field_data = defaultdict(lambda: {\n",
    "    'types': Counter(),\n",
    "    'sample_values': [],\n",
    "    'null_count': 0,\n",
    "    'total_count': 0,\n",
    "    'is_list': False,\n",
    "    'list_lengths': [],\n",
    "    'files_present': 0\n",
    "})\n",
    "\n",
    "for i, file_path in enumerate(all_files[:num_files_to_analyze], 1):\n",
    "    print(f\"Processing {i}/{num_files_to_analyze}: {file_path.name}...\", end='\\r')\n",
    "    try:\n",
    "        data = load_fotmob_json(file_path)\n",
    "        field_info = extract_all_fields(data)\n",
    "        \n",
    "        # Aggregate field information\n",
    "        for field_path, info in field_info.items():\n",
    "            all_field_data[field_path]['files_present'] += 1\n",
    "            all_field_data[field_path]['total_count'] += info['total_count']\n",
    "            all_field_data[field_path]['null_count'] += info['null_count']\n",
    "            all_field_data[field_path]['is_list'] = info['is_list']\n",
    "            all_field_data[field_path]['list_lengths'].extend(info['list_lengths'])\n",
    "            \n",
    "            for type_name in info['types']:\n",
    "                all_field_data[field_path]['types'][type_name] += 1\n",
    "            \n",
    "            if len(all_field_data[field_path]['sample_values']) < 10:\n",
    "                all_field_data[field_path]['sample_values'].extend(\n",
    "                    [v for v in info['sample_values'] \n",
    "                     if v not in all_field_data[field_path]['sample_values']][:10]\n",
    "                )\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing {file_path}: {e}\")\n",
    "\n",
    "print(f\"\\n\\n‚úì Analysis complete! Found {len(all_field_data)} unique field paths\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6cc0f3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created field analysis DataFrame with 37399 rows\n",
      "\n",
      "                                    Field Path           Type(s)  \\\n",
      "0                                                       dict(10)   \n",
      "1                                         data          dict(10)   \n",
      "2                                 data.content          dict(10)   \n",
      "3                            data.content.buzz          null(10)   \n",
      "4                             data.content.h2h  dict(9), bool(1)   \n",
      "5                     data.content.h2h.matches           list(9)   \n",
      "6                  data.content.h2h.matches[0]           dict(9)   \n",
      "7             data.content.h2h.matches[0].away           dict(9)   \n",
      "8          data.content.h2h.matches[0].away.id            str(9)   \n",
      "9        data.content.h2h.matches[0].away.name            str(9)   \n",
      "10        data.content.h2h.matches[0].finished           bool(9)   \n",
      "11            data.content.h2h.matches[0].home           dict(9)   \n",
      "12         data.content.h2h.matches[0].home.id            str(9)   \n",
      "13       data.content.h2h.matches[0].home.name            str(9)   \n",
      "14          data.content.h2h.matches[0].league           dict(9)   \n",
      "15       data.content.h2h.matches[0].league.id            str(9)   \n",
      "16     data.content.h2h.matches[0].league.name            str(9)   \n",
      "17  data.content.h2h.matches[0].league.pageUrl            str(9)   \n",
      "18        data.content.h2h.matches[0].matchUrl            str(9)   \n",
      "19          data.content.h2h.matches[0].status           dict(9)   \n",
      "\n",
      "    Files Present  Total Occurrences  Null Count  Null % Is List  \\\n",
      "0              10                 10           0    0.0%      No   \n",
      "1              10                 10           0    0.0%      No   \n",
      "2              10                 10           0    0.0%      No   \n",
      "3              10                 10          10  100.0%      No   \n",
      "4              10                 10           0    0.0%      No   \n",
      "5               9                  9           0    0.0%     Yes   \n",
      "6               9                  9           0    0.0%      No   \n",
      "7               9                  9           0    0.0%      No   \n",
      "8               9                  9           0    0.0%      No   \n",
      "9               9                  9           0    0.0%      No   \n",
      "10              9                  9           0    0.0%      No   \n",
      "11              9                  9           0    0.0%      No   \n",
      "12              9                  9           0    0.0%      No   \n",
      "13              9                  9           0    0.0%      No   \n",
      "14              9                  9           0    0.0%      No   \n",
      "15              9                  9           0    0.0%      No   \n",
      "16              9                  9           0    0.0%      No   \n",
      "17              9                  9           0    0.0%      No   \n",
      "18              9                  9           0    0.0%      No   \n",
      "19              9                  9           0    0.0%      No   \n",
      "\n",
      "                 List Info                                      Sample Values  \n",
      "0                                                                         N/A  \n",
      "1                                                                         N/A  \n",
      "2                                                                         N/A  \n",
      "3                                                                         N/A  \n",
      "4                                                                     [False]  \n",
      "5   min:1, max:19, avg:8.6                                                N/A  \n",
      "6                                                                         N/A  \n",
      "7                                                                         N/A  \n",
      "8                                              ['294582', '433030', '601595']  \n",
      "9                           ['Glasgow City', 'Washington Spirit', 'Cork Ci...  \n",
      "10                                                                    [False]  \n",
      "11                                                                        N/A  \n",
      "12                                            ['789902', '189397', '1252958']  \n",
      "13                          ['Hibernian LFC', 'NJ/NY Gotham FC', 'Athlone ...  \n",
      "14                                                                        N/A  \n",
      "15                                                 ['11019', '9134', '10210']  \n",
      "16                           ['SWPL Cup', 'NWSL', \"Women's Premier Division\"]  \n",
      "17                          ['/leagues/11019/overview/swpl-cup', '/leagues...  \n",
      "18                          ['/matches/glasgow-city-vs-hibernian-lfc/7i5c8...  \n",
      "19                                                                        N/A  \n"
     ]
    }
   ],
   "source": [
    "# Create a comprehensive DataFrame with all field information\n",
    "field_records = []\n",
    "\n",
    "for field_path, info in sorted(all_field_data.items()):\n",
    "    types_str = ', '.join(f\"{k}({v})\" for k, v in info['types'].most_common())\n",
    "    sample_str = str(info['sample_values'][:3])[:100] if info['sample_values'] else \"N/A\"\n",
    "    \n",
    "    null_percentage = (info['null_count'] / info['total_count'] * 100) if info['total_count'] > 0 else 0\n",
    "    \n",
    "    list_info = \"\"\n",
    "    if info['is_list'] and info['list_lengths']:\n",
    "        avg_len = sum(info['list_lengths']) / len(info['list_lengths'])\n",
    "        min_len = min(info['list_lengths'])\n",
    "        max_len = max(info['list_lengths'])\n",
    "        list_info = f\"min:{min_len}, max:{max_len}, avg:{avg_len:.1f}\"\n",
    "    \n",
    "    field_records.append({\n",
    "        'Field Path': field_path,\n",
    "        'Type(s)': types_str,\n",
    "        'Files Present': info['files_present'],\n",
    "        'Total Occurrences': info['total_count'],\n",
    "        'Null Count': info['null_count'],\n",
    "        'Null %': f\"{null_percentage:.1f}%\",\n",
    "        'Is List': 'Yes' if info['is_list'] else 'No',\n",
    "        'List Info': list_info,\n",
    "        'Sample Values': sample_str\n",
    "    })\n",
    "\n",
    "df_fields = pd.DataFrame(field_records)\n",
    "print(f\"Created field analysis DataFrame with {len(df_fields)} rows\\n\")\n",
    "print(df_fields.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a513d662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TOP-LEVEL STRUCTURE\n",
      "================================================================================\n",
      "Field Path  Type(s)  Files Present  Total Occurrences  Null Count Null % Is List List Info                                                                              Sample Values\n",
      "           dict(10)             10                 10           0   0.0%      No                                                                                                  N/A\n",
      "      data dict(10)             10                 10           0   0.0%      No                                                                                                  N/A\n",
      "      date  str(10)             10                 10           0   0.0%      No                                                                                         ['20251001']\n",
      "  match_id  str(10)             10                 10           0   0.0%      No                                                           ['1000002056', '1000003608', '1000003609']\n",
      "scraped_at  str(10)             10                 10           0   0.0%      No           ['2025-12-08T08:40:21.563836', '2025-12-08T08:40:04.195745', '2025-12-08T08:40:09.399679']\n",
      "\n",
      "Total top-level fields: 5\n"
     ]
    }
   ],
   "source": [
    "# Show top-level structure\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP-LEVEL STRUCTURE\")\n",
    "print(\"=\" * 80)\n",
    "top_level = df_fields[~df_fields['Field Path'].str.contains(r'\\.|\\[')]\n",
    "print(top_level.to_string(index=False))\n",
    "print(f\"\\nTotal top-level fields: {len(top_level)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86f6f466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION: data.general\n",
      "================================================================================\n",
      "\n",
      "Direct fields under data.general:\n",
      "                   Field Path  Type(s) Null % Is List                                                                                        Sample Values\n",
      "        data.general.awayTeam dict(10)   0.0%      No                                                                                                  N/A\n",
      "     data.general.countryCode  str(10)   0.0%      No                                                                                ['SCO', 'INT', 'IRL']\n",
      "   data.general.coverageLevel  str(10)   0.0%      No                                                                           ['lower', 'xG', 'ratings']\n",
      "        data.general.finished bool(10)   0.0%      No                                                                                               [True]\n",
      "        data.general.homeTeam dict(10)   0.0%      No                                                                                                  N/A\n",
      "        data.general.leagueId  int(10)   0.0%      No                                                                                [10791, 11013, 10210]\n",
      "      data.general.leagueName  str(10)   0.0%      No                          ['Scottish SWPL 1', 'Concacaf W Champions Cup', \"Women's Premier Division\"]\n",
      " data.general.leagueRoundName  str(10)   0.0%      No                                                                                     ['3', '4', '20']\n",
      "         data.general.matchId  str(10)   0.0%      No                                                           ['1000002056', '1000003608', '1000003609']\n",
      "       data.general.matchName  str(10)   0.0%      No ['Hibernian LFC-vs-Glasgow City_Wed, Oct 1, 2025, 18:35 UTC', 'CF America-vs-Orlando Pride_Wed, Oct \n",
      "      data.general.matchRound  str(10)   0.0%      No                                                                                     ['3', '4', '20']\n",
      "    data.general.matchTimeUTC  str(10)   0.0%      No        ['Wed, Oct 1, 2025, 18:35 UTC', 'Wed, Oct 1, 2025, 01:00 UTC', 'Wed, Oct 1, 2025, 23:00 UTC']\n",
      "data.general.matchTimeUTCDate  str(10)   0.0%      No                 ['2025-10-01T18:35:00.000Z', '2025-10-01T01:00:00.000Z', '2025-10-01T23:00:00.000Z']\n",
      "  data.general.parentLeagueId  int(10)   0.0%      No                                                                                [10791, 11013, 10210]\n",
      "         data.general.started bool(10)   0.0%      No                                                                                               [True]\n",
      "      data.general.teamColors dict(10)   0.0%      No                                                                                                  N/A\n",
      "\n",
      "Total fields in this section (including nested): 33\n",
      "\n",
      "================================================================================\n",
      "SECTION: data.header\n",
      "================================================================================\n",
      "\n",
      "Direct fields under data.header:\n",
      "        Field Path          Type(s) Null % Is List Sample Values\n",
      "data.header.events dict(9), null(1)  10.0%      No           N/A\n",
      "data.header.status         dict(10)   0.0%      No           N/A\n",
      " data.header.teams         list(10)   0.0%     Yes           N/A\n",
      "\n",
      "Total fields in this section (including nested): 1040\n",
      "\n",
      "================================================================================\n",
      "SECTION: data.content\n",
      "================================================================================\n",
      "\n",
      "Direct fields under data.content:\n",
      "                   Field Path          Type(s) Null % Is List Sample Values\n",
      "            data.content.buzz         null(10) 100.0%      No           N/A\n",
      "             data.content.h2h dict(9), bool(1)   0.0%      No       [False]\n",
      "      data.content.hasPlayoff         bool(10)   0.0%      No       [False]\n",
      "data.content.highlightStories          dict(1)   0.0%      No           N/A\n",
      "          data.content.lineup dict(9), null(1)  10.0%      No           N/A\n",
      "      data.content.liveticker bool(5), dict(5)   0.0%      No       [False]\n",
      "      data.content.matchFacts         dict(10)   0.0%      No           N/A\n",
      "        data.content.momentum bool(9), dict(1)   0.0%      No       [False]\n",
      "     data.content.playerStats dict(8), null(2)  20.0%      No           N/A\n",
      "         data.content.shotmap         dict(10)   0.0%      No           N/A\n",
      "           data.content.stats dict(9), null(1)  10.0%      No           N/A\n",
      "       data.content.superlive         dict(10)   0.0%      No           N/A\n",
      "           data.content.table         dict(10)   0.0%      No           N/A\n",
      "         data.content.weather          dict(8)   0.0%      No           N/A\n",
      "\n",
      "Total fields in this section (including nested): 36184\n",
      "\n",
      "================================================================================\n",
      "SECTION: data.nav\n",
      "================================================================================\n",
      "\n",
      "Direct fields under data.nav:\n",
      "Empty DataFrame\n",
      "Columns: [Field Path, Type(s), Null %, Is List, Sample Values]\n",
      "Index: []\n",
      "\n",
      "Total fields in this section (including nested): 4\n"
     ]
    }
   ],
   "source": [
    "# Explore specific sections (you can change these)\n",
    "sections_to_explore = ['data.general', 'data.header', 'data.content', 'data.nav']\n",
    "\n",
    "for section in sections_to_explore:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"SECTION: {section}\")\n",
    "    print(\"=\" * 80)\n",
    "    section_fields = df_fields[df_fields['Field Path'].str.startswith(section)]\n",
    "    \n",
    "    if len(section_fields) > 0:\n",
    "        # Show direct children only (not nested)\n",
    "        depth = section.count('.') + 1\n",
    "        direct_children = section_fields[\n",
    "            section_fields['Field Path'].apply(lambda x: x.count('.') == depth and '[' not in x.split('.')[-1])\n",
    "        ]\n",
    "        print(f\"\\nDirect fields under {section}:\")\n",
    "        print(direct_children[['Field Path', 'Type(s)', 'Null %', 'Is List', 'Sample Values']].to_string(index=False))\n",
    "        print(f\"\\nTotal fields in this section (including nested): {len(section_fields)}\")\n",
    "    else:\n",
    "        print(f\"No fields found for {section}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1081758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FIELDS WITH HIGH NULL RATES (>50%)\n",
      "================================================================================\n",
      "                                                        Field Path Null %  Files Present                                                                                        Sample Values\n",
      "             data.content.matchFacts.events.events[0].overloadTime  90.0%             10                                                                                                  [0]\n",
      "                      data.content.matchFacts.insights[2].playerId  85.7%              7                                                                                             [630365]\n",
      "            data.content.matchFacts.insights[2].statValues[0].name  85.7%              7                                                                                    ['Simen Hestnes']\n",
      "             data.content.matchFacts.events.events[0].shotmapEvent  85.7%              7                                                                                                  N/A\n",
      "            data.content.matchFacts.insights[1].statValues[0].name  85.7%              7                                                                                  ['FC Cincinnati 2']\n",
      "          data.content.matchFacts.events.events[1].goalDescription  80.0%              5                                                                                           ['Header']\n",
      "       data.content.matchFacts.events.events[1].goalDescriptionKey  80.0%              5                                                                                           ['header']\n",
      "             data.content.matchFacts.events.events[1].overloadTime  80.0%             10                                                                                               [0, 2]\n",
      "             data.content.matchFacts.events.events[1].shotmapEvent  80.0%              5                                                                                                  N/A\n",
      "                  data.content.matchFacts.playerOfTheMatch.shotmap  80.0%              5                                                                                                  N/A\n",
      "             data.content.matchFacts.events.events[2].overloadTime  70.0%             10                                                                                               [0, 3]\n",
      "         data.content.matchFacts.events.events[1].assistProfileUrl  60.0%              5                            ['/players/1047489/jonas-lange-hjorth', '/players/1078028/roberto-avila']\n",
      "data.content.matchFacts.topPlayers.awayTopPlayers[1].positionLabel  60.0%              5                                                                                                  N/A\n",
      "                data.content.matchFacts.events.events[1].assistStr  60.0%              5                                          ['assist by Jonas Lange Hjorth', 'assist by Roberto Avila']\n",
      "                               data.content.superlive.superLiveUrl  60.0%             10 ['https://pub.fotmob.com/prod/news/api/law?matchid=1q7wp7gicdprvdrriuvbnqfx0&competition=9ynnnx1qmki\n",
      "                        data.content.matchFacts.infoBox.Attendance  60.0%             10                                                                                   [1539, 4049, 4429]\n",
      "                data.content.matchFacts.events.events[2].player.id  60.0%             10                                                                          [1716798, 1727057, 1054345]\n",
      "                                data.content.matchFacts.highlights  60.0%             10                                                                                                  N/A\n",
      "               data.header.events.awayTeamGoals.Brady[0].assistStr 100.0%              1                                                                                                  N/A\n",
      "            data.header.events.awayTeamGoals.Brady[0].overloadTime 100.0%              1                                                                                                  N/A\n",
      "      data.header.events.awayTeamGoals.Brady[0].goalDescriptionKey 100.0%              1                                                                                                  N/A\n",
      "                 data.header.events.awayTeamGoals.Brady[0].ownGoal 100.0%              1                                                                                                  N/A\n",
      "        data.header.events.awayTeamGoals.Brady[0].penShootoutScore 100.0%              1                                                                                                  N/A\n",
      "            data.header.events.awayTeamGoals.Brady[0].shotmapEvent 100.0%              1                                                                                                  N/A\n",
      "                  data.header.events.awayTeamGoals.Brady[0].suffix 100.0%              1                                                                                                  N/A\n",
      "         data.header.events.awayTeamGoals.Brady[0].goalDescription 100.0%              1                                                                                                  N/A\n",
      "               data.header.events.awayTeamGoals.Brady[0].suffixKey 100.0%              1                                                                                                  N/A\n",
      "                                                 data.content.buzz 100.0%             10                                                                                                  N/A\n",
      "        data.header.events.awayTeamGoals.Brady[0].assistProfileUrl 100.0%              1                                                                                                  N/A\n",
      "               data.header.events.awayTeamGoals.Brady[1].assistStr 100.0%              1                                                                                                  N/A\n"
     ]
    }
   ],
   "source": [
    "# Find fields with high null rates (potential issues)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FIELDS WITH HIGH NULL RATES (>50%)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "high_null_fields = df_fields[\n",
    "    df_fields['Null %'].str.rstrip('%').astype(float) > 50\n",
    "].sort_values('Null %', ascending=False)\n",
    "\n",
    "if len(high_null_fields) > 0:\n",
    "    print(high_null_fields[['Field Path', 'Null %', 'Files Present', 'Sample Values']].head(30).to_string(index=False))\n",
    "else:\n",
    "    print(\"No fields with high null rates found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34dc21d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Use search_fields('keyword') to find specific fields!\n"
     ]
    }
   ],
   "source": [
    "# Search for specific fields by keyword\n",
    "def search_fields(keyword: str, case_sensitive: bool = False):\n",
    "    \"\"\"Search for fields containing a keyword.\"\"\"\n",
    "    if case_sensitive:\n",
    "        matches = df_fields[df_fields['Field Path'].str.contains(keyword)]\n",
    "    else:\n",
    "        matches = df_fields[df_fields['Field Path'].str.lower().str.contains(keyword.lower())]\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"SEARCH RESULTS FOR: '{keyword}' (Found {len(matches)} matches)\")\n",
    "    print('=' * 80)\n",
    "    \n",
    "    if len(matches) > 0:\n",
    "        print(matches[['Field Path', 'Type(s)', 'Null %', 'Is List', 'Sample Values']].to_string(index=False))\n",
    "    else:\n",
    "        print(f\"No fields found containing '{keyword}'\")\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# Example searches - uncomment and modify as needed\n",
    "# search_fields('player')\n",
    "# search_fields('goal')\n",
    "# search_fields('xg')\n",
    "# search_fields('lineup')\n",
    "\n",
    "print(\"\\nUse search_fields('keyword') to find specific fields!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f1a2af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading detailed view of: match_1000002056.json.gz\n",
      "\n",
      "================================================================================\n",
      "SAMPLE FILE STRUCTURE\n",
      "================================================================================\n",
      "match_id: str = 1000002056\n",
      "scraped_at: str = 2025-12-08T08:40:21.563836\n",
      "date: str = 20251001\n",
      "data: {dict with 7 keys}\n",
      "  general: {dict with 16 keys}\n",
      "    matchId: str = 1000002056\n",
      "    matchName: str = Hibernian LFC-vs-Glasgow City_Wed, Oct 1, 2025, 18\n",
      "    matchRound: str = 3\n",
      "    teamColors: {dict with 4 keys}\n",
      "    leagueId: int = 10791\n",
      "    leagueName: str = Scottish SWPL 1\n",
      "    leagueRoundName: str = 3\n",
      "    parentLeagueId: int = 10791\n",
      "    countryCode: str = SCO\n",
      "    homeTeam: {dict with 2 keys}\n",
      "    awayTeam: {dict with 2 keys}\n",
      "    coverageLevel: str = lower\n",
      "    matchTimeUTC: str = Wed, Oct 1, 2025, 18:35 UTC\n",
      "    matchTimeUTCDate: str = 2025-10-01T18:35:00.000Z\n",
      "    started: bool = True\n",
      "    finished: bool = True\n",
      "  header: {dict with 3 keys}\n",
      "    teams: [list with 2 items]\n",
      "      Item type: dict\n",
      "    status: {dict with 12 keys}\n",
      "    events: {dict with 4 keys}\n",
      "  nav: [list with 5 items]\n",
      "    Item type: str\n",
      "  ongoing: bool = False\n",
      "  hasPendingVAR: bool = False\n",
      "  content: {dict with 13 keys}\n",
      "    matchFacts: {dict with 10 keys}\n",
      "    liveticker: bool = False\n",
      "    superlive: {dict with 2 keys}\n",
      "    buzz: NoneType = None\n",
      "    stats: {dict with 1 keys}\n",
      "    playerStats: {dict with 39 keys}\n",
      "    shotmap: {dict with 2 keys}\n",
      "    weather: {dict with 13 keys}\n",
      "    lineup: {dict with 6 keys}\n",
      "    hasPlayoff: bool = False\n",
      "    table: {dict with 6 keys}\n",
      "    h2h: {dict with 2 keys}\n",
      "    momentum: bool = False\n",
      "  seo: {dict with 4 keys}\n",
      "    path: str = hibernian-lfc-vs-glasgow-city\n",
      "    eventJSONLD: {dict with 16 keys}\n",
      "    breadcrumbJSONLD: [list with 3 items]\n",
      "      Item type: dict\n",
      "    faqJSONLD: {dict with 3 keys}\n"
     ]
    }
   ],
   "source": [
    "# Load a single file for detailed inspection\n",
    "sample_file = all_files[0] if all_files else None\n",
    "\n",
    "if sample_file:\n",
    "    print(f\"Loading detailed view of: {sample_file.name}\\n\")\n",
    "    sample_data = load_fotmob_json(sample_file)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"SAMPLE FILE STRUCTURE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    def print_structure(obj, indent=0, max_depth=3, current_depth=0):\n",
    "        \"\"\"Print nested structure with indentation.\"\"\"\n",
    "        if current_depth >= max_depth:\n",
    "            return\n",
    "        \n",
    "        prefix = \"  \" * indent\n",
    "        \n",
    "        if isinstance(obj, dict):\n",
    "            for key, value in list(obj.items())[:20]:  # Limit to first 20 keys\n",
    "                if isinstance(value, dict):\n",
    "                    print(f\"{prefix}{key}: {{dict with {len(value)} keys}}\")\n",
    "                    print_structure(value, indent+1, max_depth, current_depth+1)\n",
    "                elif isinstance(value, list):\n",
    "                    print(f\"{prefix}{key}: [list with {len(value)} items]\")\n",
    "                    if len(value) > 0:\n",
    "                        print(f\"{prefix}  Item type: {type(value[0]).__name__}\")\n",
    "                        if isinstance(value[0], dict):\n",
    "                            print_structure(value[0], indent+2, max_depth, current_depth+1)\n",
    "                else:\n",
    "                    val_str = str(value)[:50]\n",
    "                    print(f\"{prefix}{key}: {type(value).__name__} = {val_str}\")\n",
    "        elif isinstance(obj, list) and len(obj) > 0:\n",
    "            print(f\"{prefix}[0]: {type(obj[0]).__name__}\")\n",
    "            print_structure(obj[0], indent+1, max_depth, current_depth+1)\n",
    "    \n",
    "    print_structure(sample_data)\n",
    "else:\n",
    "    print(\"No sample file available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a5eb4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚úì Full field analysis exported to: fotmob_field_analysis.csv\n",
      "================================================================================\n",
      "\n",
      "üìä SUMMARY STATISTICS:\n",
      "  Total unique fields analyzed: 37399\n",
      "  Files analyzed: 10\n",
      "  Fields that are lists: 870\n",
      "  Fields with >50% null rate: 622\n",
      "  Most common data types: {'str(1)': 13618, 'dict(1)': 12536, 'int(1)': 7011, 'list(1)': 771, 'bool(1)': 662}\n"
     ]
    }
   ],
   "source": [
    "# Export full field list to CSV for easier review\n",
    "output_file = \"fotmob_field_analysis.csv\"\n",
    "df_fields.to_csv(output_file, index=False)\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"‚úì Full field analysis exported to: {output_file}\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìä SUMMARY STATISTICS:\")\n",
    "print(f\"  Total unique fields analyzed: {len(df_fields)}\")\n",
    "print(f\"  Files analyzed: {num_files_to_analyze}\")\n",
    "print(f\"  Fields that are lists: {len(df_fields[df_fields['Is List'] == 'Yes'])}\")\n",
    "print(f\"  Fields with >50% null rate: {len(high_null_fields)}\")\n",
    "print(f\"  Most common data types: {df_fields['Type(s)'].value_counts().head(5).to_dict()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "916580c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "IMPORTANT SECTIONS BREAKDOWN\n",
      "================================================================================\n",
      "\n",
      "üìÅ Match Metadata (data.general)\n",
      "   Total fields: 33\n",
      "   Top types: {'str(10)': 19, 'dict(10)': 8, 'int(10)': 4}\n",
      "   Direct fields: awayTeam, countryCode, coverageLevel, finished, homeTeam, leagueId, leagueName, leagueRoundName, matchId, matchName\n",
      "\n",
      "üìÅ Match Header (data.header)\n",
      "   Total fields: 1040\n",
      "   Top types: {'str(1)': 317, 'int(1)': 255, 'null(1)': 212}\n",
      "   Direct fields: events, status, teams\n",
      "\n",
      "üìÅ Teams (data.header.teams)\n",
      "   Total fields: 15\n",
      "   Top types: {'str(10)': 6, 'int(10)': 4, 'dict(10)': 2}\n",
      "   Direct fields: fifaRank, id, imageUrl, name, pageUrl, score, fifaRank, id, imageUrl, name\n",
      "\n",
      "üìÅ Events (data.header.events)\n",
      "   Total fields: 1000\n",
      "   Top types: {'str(1)': 317, 'int(1)': 255, 'null(1)': 212}\n",
      "   Direct fields: awayTeamGoals, awayTeamRedCards, homeTeamGoals, homeTeamRedCards\n",
      "\n",
      "üìÅ Content Stats (data.content.stats)\n",
      "   Total fields: 320\n",
      "   Top types: {'str(8)': 152, 'dict(8)': 44, 'list(8)': 34}\n",
      "   Direct fields: Periods\n",
      "\n",
      "üìÅ Player Stats (data.content.stats.players)\n",
      "   Total fields: 0\n",
      "\n",
      "üìÅ Lineup (data.content.lineup)\n",
      "   Total fields: 455\n",
      "   Top types: {'str(9)': 79, 'str(1)': 47, 'float(9)': 47}\n",
      "   Direct fields: availableFilters, awayTeam, homeTeam, lineupType, matchId, source\n",
      "\n",
      "üìÅ Shotmap (data.content.shotmap)\n",
      "   Total fields: 402\n",
      "   Top types: {'str(1)': 108, 'float(1)': 101, 'int(1)': 66}\n",
      "   Direct fields: Periods, shots\n",
      "\n",
      "üìÅ Momentum (data.content.momentum)\n",
      "   Total fields: 14\n",
      "   Top types: {'int(1)': 6, 'dict(1)': 4, 'list(1)': 2}\n",
      "   Direct fields: alternateModels, main\n"
     ]
    }
   ],
   "source": [
    "# Deep dive into specific important sections\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"IMPORTANT SECTIONS BREAKDOWN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "important_sections = {\n",
    "    'Match Metadata': 'data.general',\n",
    "    'Match Header': 'data.header',\n",
    "    'Teams': 'data.header.teams',\n",
    "    'Events': 'data.header.events',\n",
    "    'Content Stats': 'data.content.stats',\n",
    "    'Player Stats': 'data.content.stats.players',\n",
    "    'Lineup': 'data.content.lineup',\n",
    "    'Shotmap': 'data.content.shotmap',\n",
    "    'Momentum': 'data.content.momentum'\n",
    "}\n",
    "\n",
    "for section_name, section_path in important_sections.items():\n",
    "    section_data = df_fields[df_fields['Field Path'].str.startswith(section_path)]\n",
    "    print(f\"\\nüìÅ {section_name} ({section_path})\")\n",
    "    print(f\"   Total fields: {len(section_data)}\")\n",
    "    \n",
    "    if len(section_data) > 0:\n",
    "        # Count field types\n",
    "        field_types = section_data['Type(s)'].value_counts().head(3)\n",
    "        print(f\"   Top types: {dict(field_types)}\")\n",
    "        \n",
    "        # Show sample of direct children\n",
    "        depth = section_path.count('.') + 1\n",
    "        direct = section_data[\n",
    "            section_data['Field Path'].apply(lambda x: x.count('.') == depth and '[' not in x.split('.')[-1])\n",
    "        ]\n",
    "        if len(direct) > 0:\n",
    "            print(f\"   Direct fields: {', '.join(direct['Field Path'].str.split('.').str[-1].head(10).tolist())}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7633cabe",
   "metadata": {},
   "source": [
    "# üîç FotMob Field Inspector - Interactive Guide\n",
    "\n",
    "This notebook provides comprehensive analysis of all FotMob JSON fields. Use it to:\n",
    "\n",
    "1. **Discover all fields** - See every field path across all match files\n",
    "2. **Find missing/null data** - Identify fields with high null rates\n",
    "3. **Search for specific fields** - Use `search_fields('keyword')` function\n",
    "4. **Understand data types** - See what types each field contains\n",
    "5. **Inspect sample values** - View example data for each field\n",
    "6. **Export to CSV** - Full analysis saved to `fotmob_field_analysis.csv`\n",
    "\n",
    "## Common Search Examples:\n",
    "```python\n",
    "# Search for player-related fields\n",
    "search_fields('player')\n",
    "\n",
    "# Search for goal/scoring data\n",
    "search_fields('goal')\n",
    "\n",
    "# Search for xG (expected goals) data\n",
    "search_fields('xg')\n",
    "\n",
    "# Search for lineup information\n",
    "search_fields('lineup')\n",
    "\n",
    "# Search for stats\n",
    "search_fields('stats')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e906eb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FIELD CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "Fields not present in ALL 10 files:\n",
      "Total inconsistent fields: 36903\n",
      "\n",
      "                                                                        Field Path  Files Present Type(s) Null %\n",
      "                                    data.content.playerStats.1624169.usualPosition              1  int(1)   0.0%\n",
      "        data.content.playerStats.1781532.stats[1].stats.Shots on target.stat.total              1  int(1)   0.0%\n",
      "              data.content.playerStats.1781532.stats[1].stats.Shots on target.stat              1 dict(1)   0.0%\n",
      "               data.content.playerStats.1781532.stats[1].stats.Shots on target.key              1  str(1)   0.0%\n",
      "                   data.content.playerStats.1781532.stats[1].stats.Shots on target              1 dict(1)   0.0%\n",
      "data.content.playerStats.1781532.stats[1].stats.Passes into final third.stat.value              1  int(1)   0.0%\n",
      " data.content.playerStats.1781532.stats[1].stats.Passes into final third.stat.type              1  str(1)   0.0%\n",
      "      data.content.playerStats.1781532.stats[1].stats.Passes into final third.stat              1 dict(1)   0.0%\n",
      "       data.content.playerStats.1781532.stats[1].stats.Passes into final third.key              1  str(1)   0.0%\n",
      "           data.content.playerStats.1781532.stats[1].stats.Passes into final third              1 dict(1)   0.0%\n",
      "           data.content.playerStats.1781532.stats[1].stats.Dispossessed.stat.value              1  int(1)   0.0%\n",
      "            data.content.playerStats.1781532.stats[1].stats.Dispossessed.stat.type              1  str(1)   0.0%\n",
      "                 data.content.playerStats.1781532.stats[1].stats.Dispossessed.stat              1 dict(1)   0.0%\n",
      "                  data.content.playerStats.1781532.stats[1].stats.Dispossessed.key              1  str(1)   0.0%\n",
      "                      data.content.playerStats.1781532.stats[1].stats.Dispossessed              1 dict(1)   0.0%\n",
      "          data.content.playerStats.1781532.stats[1].stats.Blocked shots.stat.value              1  int(1)   0.0%\n",
      "           data.content.playerStats.1781532.stats[1].stats.Blocked shots.stat.type              1  str(1)   0.0%\n",
      "                data.content.playerStats.1781532.stats[1].stats.Blocked shots.stat              1 dict(1)   0.0%\n",
      "             data.content.playerStats.1781532.stats[0].stats.Total shots.stat.type              1  str(1)   0.0%\n",
      "            data.content.playerStats.1781532.stats[0].stats.Total shots.stat.value              1  int(1)   0.0%\n",
      "\n",
      "‚ö†Ô∏è Warning: 36903 fields are not present in all files\n",
      "   This could indicate:\n",
      "   - Optional fields\n",
      "   - Fields that depend on match type/league\n",
      "   - Missing data that should be investigated\n"
     ]
    }
   ],
   "source": [
    "# Compare fields across different matches to find inconsistencies\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FIELD CONSISTENCY CHECK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Fields that don't appear in all files\n",
    "inconsistent_fields = df_fields[df_fields['Files Present'] < num_files_to_analyze]\n",
    "print(f\"\\nFields not present in ALL {num_files_to_analyze} files:\")\n",
    "print(f\"Total inconsistent fields: {len(inconsistent_fields)}\\n\")\n",
    "\n",
    "if len(inconsistent_fields) > 0:\n",
    "    # Sort by how many files are missing this field\n",
    "    inconsistent_fields_sorted = inconsistent_fields.sort_values('Files Present')\n",
    "    print(inconsistent_fields_sorted[['Field Path', 'Files Present', 'Type(s)', 'Null %']].head(20).to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è Warning: {len(inconsistent_fields)} fields are not present in all files\")\n",
    "    print(f\"   This could indicate:\")\n",
    "    print(f\"   - Optional fields\")\n",
    "    print(f\"   - Fields that depend on match type/league\")\n",
    "    print(f\"   - Missing data that should be investigated\")\n",
    "else:\n",
    "    print(\"‚úì All fields are present in all analyzed files!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e274f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° TIP: Use browse_section('path.to.section') to explore any section!\n"
     ]
    }
   ],
   "source": [
    "# Interactive field browser - modify the path to explore different sections\n",
    "def browse_section(path: str, max_children: int = 50):\n",
    "    \"\"\"Browse a specific section of the JSON structure.\"\"\"\n",
    "    section_fields = df_fields[df_fields['Field Path'].str.startswith(path)]\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"BROWSING: {path}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    if len(section_fields) == 0:\n",
    "        print(f\"‚ùå No fields found for path: {path}\")\n",
    "        print(\"\\nDid you mean one of these?\")\n",
    "        # Find similar paths\n",
    "        similar = df_fields[df_fields['Field Path'].str.contains(path.split('.')[-1], case=False)]\n",
    "        if len(similar) > 0:\n",
    "            print(similar['Field Path'].head(10).tolist())\n",
    "        return\n",
    "    \n",
    "    # Show direct children\n",
    "    depth = path.count('.') + 1\n",
    "    direct_children = section_fields[\n",
    "        section_fields['Field Path'].apply(lambda x: x.count('.') == depth and '[' not in x.split('.')[-1])\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüìä Section Overview:\")\n",
    "    print(f\"   Total fields (including nested): {len(section_fields)}\")\n",
    "    print(f\"   Direct children: {len(direct_children)}\")\n",
    "    \n",
    "    if len(direct_children) > 0:\n",
    "        print(f\"\\nüìã Direct Children ({min(len(direct_children), max_children)} shown):\")\n",
    "        display_cols = ['Field Path', 'Type(s)', 'Null %', 'Is List', 'List Info', 'Sample Values']\n",
    "        print(direct_children[display_cols].head(max_children).to_string(index=False))\n",
    "    \n",
    "    # Show nested structures\n",
    "    nested = section_fields[\n",
    "        section_fields['Field Path'].apply(lambda x: x.count('.') > depth)\n",
    "    ]\n",
    "    if len(nested) > 0:\n",
    "        print(f\"\\nüå≥ Nested Structures: {len(nested)} additional nested fields\")\n",
    "        # Group by parent path\n",
    "        nested_parents = nested['Field Path'].apply(\n",
    "            lambda x: '.'.join(x.split('.')[:depth+1])\n",
    "        ).unique()\n",
    "        print(f\"   Nested under: {list(nested_parents[:10])}\")\n",
    "    \n",
    "    return direct_children\n",
    "\n",
    "# Example: Browse different sections (uncomment to use)\n",
    "# browse_section('data')\n",
    "# browse_section('data.general')\n",
    "# browse_section('data.content.lineup')\n",
    "# browse_section('data.content.shotmap')\n",
    "# browse_section('data.content.stats')\n",
    "\n",
    "print(\"\\nüí° TIP: Use browse_section('path.to.section') to explore any section!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b67bf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° TIP: Use inspect_match(file_index=N) or inspect_match(match_id='ID') to inspect specific matches!\n"
     ]
    }
   ],
   "source": [
    "# Inspect specific match for debugging\n",
    "def inspect_match(match_id: str = None, file_index: int = 0):\n",
    "    \"\"\"Load and inspect a specific match in detail.\"\"\"\n",
    "    if match_id:\n",
    "        # Find file with this match_id\n",
    "        target_file = None\n",
    "        for f in all_files:\n",
    "            if f\"match_{match_id}\" in f.name:\n",
    "                target_file = f\n",
    "                break\n",
    "        if not target_file:\n",
    "            print(f\"‚ùå Match {match_id} not found\")\n",
    "            return None\n",
    "    else:\n",
    "        if file_index >= len(all_files):\n",
    "            print(f\"‚ùå File index {file_index} out of range (max: {len(all_files)-1})\")\n",
    "            return None\n",
    "        target_file = all_files[file_index]\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"INSPECTING: {target_file.name}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    data = load_fotmob_json(target_file)\n",
    "    \n",
    "    # Show basic match info\n",
    "    if 'data' in data and 'general' in data['data']:\n",
    "        gen = data['data']['general']\n",
    "        print(f\"\\nüìã Match Info:\")\n",
    "        print(f\"   Match ID: {gen.get('matchId', 'N/A')}\")\n",
    "        print(f\"   Match: {gen.get('matchName', 'N/A')}\")\n",
    "        print(f\"   League: {gen.get('leagueName', 'N/A')}\")\n",
    "        print(f\"   Home: {gen.get('homeTeam', {}).get('name', 'N/A')}\")\n",
    "        print(f\"   Away: {gen.get('awayTeam', {}).get('name', 'N/A')}\")\n",
    "        print(f\"   Status: {'Finished' if gen.get('finished') else 'Not Finished'}\")\n",
    "    \n",
    "    # Show available top-level sections\n",
    "    if 'data' in data:\n",
    "        sections = list(data['data'].keys())\n",
    "        print(f\"\\nüìÇ Available Sections: {len(sections)}\")\n",
    "        for section in sections:\n",
    "            section_data = data['data'][section]\n",
    "            if isinstance(section_data, dict):\n",
    "                print(f\"   ‚Ä¢ {section}: dict with {len(section_data)} keys\")\n",
    "            elif isinstance(section_data, list):\n",
    "                print(f\"   ‚Ä¢ {section}: list with {len(section_data)} items\")\n",
    "            else:\n",
    "                print(f\"   ‚Ä¢ {section}: {type(section_data).__name__}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Example usage:\n",
    "# match_data = inspect_match(file_index=0)  # Inspect first file\n",
    "# match_data = inspect_match(match_id='4947772')  # Inspect specific match\n",
    "\n",
    "print(\"\\nüí° TIP: Use inspect_match(file_index=N) or inspect_match(match_id='ID') to inspect specific matches!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d16be81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üîß QUICK TROUBLESHOOTING CHECKS\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ Required Fields Check:\n",
      "   ‚úì data.general.matchId: 10/10 files, 0.0% null\n",
      "   ‚úì data.general.homeTeam: 10/10 files, 0.0% null\n",
      "   ‚úì data.general.awayTeam: 10/10 files, 0.0% null\n",
      "   ‚úì data.header.teams: 10/10 files, 0.0% null\n",
      "   ‚úì data.header.status: 10/10 files, 0.0% null\n",
      "\n",
      "2Ô∏è‚É£ Variable Length Lists (potential data quality issues):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                             Field Path              List Info  Files Present\n",
      "                                               data.content.h2h.matches min:1, max:19, avg:8.6              9\n",
      "                    data.content.h2h.matches[0].status.reason.penalties  min:2, max:2, avg:2.0              1\n",
      "                    data.content.h2h.matches[1].status.reason.penalties  min:2, max:2, avg:2.0              1\n",
      "                    data.content.h2h.matches[2].status.reason.penalties  min:2, max:2, avg:2.0              1\n",
      "                                               data.content.h2h.summary  min:3, max:3, avg:3.0              9\n",
      "                                  data.content.highlightStories.stories  min:1, max:1, avg:1.0              1\n",
      "                       data.content.highlightStories.stories[0].content  min:1, max:1, avg:1.0              1\n",
      "data.content.highlightStories.stories[0].content[0].restriction.allowed  min:0, max:0, avg:0.0              1\n",
      "data.content.highlightStories.stories[0].content[0].restriction.blocked  min:0, max:0, avg:0.0              1\n",
      "                                   data.content.lineup.availableFilters  min:3, max:5, avg:3.3              9\n",
      "\n",
      "3Ô∏è‚É£ Fields with Multiple Types (data consistency check):\n",
      "   Found 54 fields with multiple types\n",
      "                                                 Field Path          Type(s)  Files Present\n",
      "                                           data.content.h2h dict(9), bool(1)             10\n",
      "                                        data.content.lineup dict(9), null(1)             10\n",
      "             data.content.lineup.awayTeam.averageStarterAge float(8), int(1)              9\n",
      "                        data.content.lineup.awayTeam.rating float(5), int(3)              8\n",
      "data.content.lineup.awayTeam.starters[1].performance.rating float(4), int(1)              5\n",
      "    data.content.lineup.awayTeam.subs[1].performance.rating float(3), int(1)              4\n",
      "                        data.content.lineup.homeTeam.rating int(4), float(4)              8\n",
      "                                    data.content.liveticker bool(5), dict(5)             10\n",
      "  data.content.matchFacts.events.events[0].assistProfileUrl  str(4), null(3)              7\n",
      "         data.content.matchFacts.events.events[0].assistStr  str(4), null(3)              7\n",
      "\n",
      "4Ô∏è‚É£ Frequently Empty Lists:\n",
      "                                                             Field Path             List Info  Files Present\n",
      "data.content.highlightStories.stories[0].content[0].restriction.allowed min:0, max:0, avg:0.0              1\n",
      "data.content.highlightStories.stories[0].content[0].restriction.blocked min:0, max:0, avg:0.0              1\n",
      "                       data.content.matchFacts.momentum.alternateModels min:0, max:0, avg:0.0              1\n",
      "              data.content.matchFacts.poll.oddspoll.Facts[0].StatValues min:0, max:8, avg:3.8             10\n",
      "                      data.content.matchFacts.topPlayers.awayTopPlayers min:0, max:5, avg:2.5             10\n",
      "                      data.content.matchFacts.topPlayers.homeTopPlayers min:0, max:5, avg:2.5             10\n",
      "                                  data.content.momentum.alternateModels min:0, max:0, avg:0.0              1\n",
      "                              data.content.playerStats.1026134.funFacts min:0, max:0, avg:0.0              1\n",
      "                              data.content.playerStats.1036400.funFacts min:0, max:0, avg:0.0              1\n",
      "                              data.content.playerStats.1042921.funFacts min:0, max:0, avg:0.0              1\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Quick troubleshooting - common issues and checks\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîß QUICK TROUBLESHOOTING CHECKS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check 1: Required fields presence\n",
    "required_fields = [\n",
    "    'data.general.matchId',\n",
    "    'data.general.homeTeam',\n",
    "    'data.general.awayTeam',\n",
    "    'data.header.teams',\n",
    "    'data.header.status'\n",
    "]\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ Required Fields Check:\")\n",
    "for field in required_fields:\n",
    "    field_data = df_fields[df_fields['Field Path'] == field]\n",
    "    if len(field_data) > 0:\n",
    "        null_pct = field_data.iloc[0]['Null %']\n",
    "        files_present = field_data.iloc[0]['Files Present']\n",
    "        print(f\"   ‚úì {field}: {files_present}/{num_files_to_analyze} files, {null_pct} null\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {field}: NOT FOUND\")\n",
    "\n",
    "# Check 2: List fields with variable lengths\n",
    "print(\"\\n2Ô∏è‚É£ Variable Length Lists (potential data quality issues):\")\n",
    "variable_lists = df_fields[\n",
    "    (df_fields['Is List'] == 'Yes') & \n",
    "    (df_fields['List Info'] != '')\n",
    "]\n",
    "if len(variable_lists) > 0:\n",
    "    print(variable_lists[['Field Path', 'List Info', 'Files Present']].head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"   No variable length lists found\")\n",
    "\n",
    "# Check 3: Fields with multiple types (inconsistent data)\n",
    "print(\"\\n3Ô∏è‚É£ Fields with Multiple Types (data consistency check):\")\n",
    "multi_type_fields = df_fields[df_fields['Type(s)'].str.contains(',')]\n",
    "if len(multi_type_fields) > 0:\n",
    "    print(f\"   Found {len(multi_type_fields)} fields with multiple types\")\n",
    "    print(multi_type_fields[['Field Path', 'Type(s)', 'Files Present']].head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"   ‚úì All fields have consistent types\")\n",
    "\n",
    "# Check 4: Empty lists\n",
    "print(\"\\n4Ô∏è‚É£ Frequently Empty Lists:\")\n",
    "empty_lists = variable_lists[variable_lists['List Info'].str.contains('min:0')]\n",
    "if len(empty_lists) > 0:\n",
    "    print(empty_lists[['Field Path', 'List Info', 'Files Present']].head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"   No frequently empty lists found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402c8a77",
   "metadata": {},
   "source": [
    "## üìù Summary & Next Steps\n",
    "\n",
    "This notebook has analyzed the FotMob JSON structure. Here's what you can do:\n",
    "\n",
    "### Immediate Actions:\n",
    "1. **Review the CSV export** - Open `fotmob_field_analysis.csv` for a complete field list\n",
    "2. **Check high null rate fields** - Review fields with >50% null to see if this is expected\n",
    "3. **Verify required fields** - Make sure critical fields exist in all files\n",
    "4. **Investigate inconsistencies** - Look at fields not present in all files\n",
    "\n",
    "### Common Use Cases:\n",
    "\n",
    "#### Find all player-related fields:\n",
    "```python\n",
    "search_fields('player')\n",
    "```\n",
    "\n",
    "#### Browse lineup structure:\n",
    "```python\n",
    "browse_section('data.content.lineup')\n",
    "```\n",
    "\n",
    "#### Inspect a specific match:\n",
    "```python\n",
    "match_data = inspect_match(match_id='4947772')\n",
    "# Then explore: match_data['data']['content']['shotmap']\n",
    "```\n",
    "\n",
    "#### Check for missing data:\n",
    "```python\n",
    "# Fields with >80% null rate\n",
    "critical_nulls = df_fields[df_fields['Null %'].str.rstrip('%').astype(float) > 80]\n",
    "print(critical_nulls[['Field Path', 'Null %']])\n",
    "```\n",
    "\n",
    "### Data Quality Checks:\n",
    "- ‚úÖ All files load successfully\n",
    "- ‚úÖ Required metadata fields present\n",
    "- ‚ö†Ô∏è Check fields not present in all files\n",
    "- ‚ö†Ô∏è Review fields with high null rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a873a7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä CURRENT ANALYSIS SCOPE\n",
      "================================================================================\n",
      "Files analyzed: 10\n",
      "Total files available: 150\n",
      "Coverage: 6.7%\n",
      "\n",
      "To analyze more files, adjust 'num_files_to_analyze' in cell 3 and re-run cells 3-17\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: Analyze more files for better coverage\n",
    "# Uncomment and run this cell to analyze ALL FotMob files (will take longer)\n",
    "\n",
    "# print(f\"Total files available: {len(all_files)}\")\n",
    "# num_to_analyze = int(input(f\"How many files to analyze? (1-{len(all_files)}): \"))\n",
    "# Then re-run cells 3-17 with the new num_files_to_analyze value\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"üìä CURRENT ANALYSIS SCOPE\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"Files analyzed: {num_files_to_analyze}\")\n",
    "print(f\"Total files available: {len(all_files)}\")\n",
    "print(f\"Coverage: {num_files_to_analyze/len(all_files)*100:.1f}%\")\n",
    "print(f\"\\nTo analyze more files, adjust 'num_files_to_analyze' in cell 3 and re-run cells 3-17\")\n",
    "print(f\"{'=' * 80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f5aa26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîß Field Mapping & Validation Fixes\n",
    "\n",
    "This section identifies missing fields, naming mismatches, and provides fixes for the match processor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "770e323f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Expected field mappings loaded\n"
     ]
    }
   ],
   "source": [
    "# Define expected field mappings based on match processor\n",
    "EXPECTED_FIELD_MAPPINGS = {\n",
    "    'general': {\n",
    "        'match_id': 'data.general.matchId',\n",
    "        'match_name': 'data.general.matchName',\n",
    "        'league_id': 'data.general.leagueId',\n",
    "        'league_name': 'data.general.leagueName',\n",
    "        'home_team_id': 'data.general.homeTeam.id',\n",
    "        'home_team_name': 'data.general.homeTeam.name',\n",
    "        'away_team_id': 'data.general.awayTeam.id',\n",
    "        'away_team_name': 'data.general.awayTeam.name',\n",
    "        'match_time_utc': 'data.general.matchTimeUTC',\n",
    "        'started': 'data.general.started',\n",
    "        'finished': 'data.general.finished',\n",
    "    },\n",
    "    'header_status': {\n",
    "        'utc_time': 'data.header.status.utcTime',\n",
    "        'finished': 'data.header.status.finished',\n",
    "        'started': 'data.header.status.started',\n",
    "        'cancelled': 'data.header.status.cancelled',\n",
    "        'score_str': 'data.header.status.scoreStr',\n",
    "    },\n",
    "    'timeline': {\n",
    "        'first_half_started': 'data.header.status.halfs.firstHalfStarted',\n",
    "        'first_half_ended': 'data.header.status.halfs.firstHalfEnded',\n",
    "        'second_half_started': 'data.header.status.halfs.secondHalfStarted',\n",
    "        'second_half_ended': 'data.header.status.halfs.secondHalfEnded',\n",
    "        'game_ended': 'data.header.status.halfs.gameEnded',\n",
    "    },\n",
    "    'goals': {\n",
    "        'home_team_goals': 'data.header.events.homeTeamGoals',\n",
    "        'away_team_goals': 'data.header.events.awayTeamGoals',\n",
    "    },\n",
    "    'content': {\n",
    "        'match_facts': 'data.content.matchFacts',\n",
    "        'stats': 'data.content.stats',\n",
    "        'lineup': 'data.content.lineup',\n",
    "        'shotmap': 'data.content.shotmap',\n",
    "        'momentum': 'data.content.momentum',\n",
    "        'player_stats': 'data.content.playerStats',\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úì Expected field mappings loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b74de8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FIELD MAPPING VALIDATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "Total expected fields: 29\n",
      "Present: 29\n",
      "Missing: 0\n",
      "Similar: 0\n"
     ]
    }
   ],
   "source": [
    "# Check which expected fields are present in actual data\n",
    "def validate_field_mappings(expected_mappings: dict, actual_fields_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Check if expected fields exist in actual data.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for category, fields in expected_mappings.items():\n",
    "        for field_name, field_path in fields.items():\n",
    "            # Check if field exists in actual data\n",
    "            matching = actual_fields_df[actual_fields_df['Field Path'] == field_path]\n",
    "            \n",
    "            if len(matching) > 0:\n",
    "                row = matching.iloc[0]\n",
    "                status = \"‚úì Present\"\n",
    "                null_pct = row['Null %']\n",
    "                types = row['Type(s)']\n",
    "                sample = str(row['Sample Values'])[:50]\n",
    "            else:\n",
    "                # Try to find similar fields\n",
    "                similar = actual_fields_df[\n",
    "                    actual_fields_df['Field Path'].str.contains(\n",
    "                        field_path.split('.')[-1], case=False, na=False\n",
    "                    )\n",
    "                ]\n",
    "                if len(similar) > 0:\n",
    "                    status = \"‚ö†Ô∏è Similar found\"\n",
    "                    null_pct = f\"See: {similar.iloc[0]['Field Path']}\"\n",
    "                    types = \"\"\n",
    "                    sample = \"\"\n",
    "                else:\n",
    "                    status = \"‚ùå Missing\"\n",
    "                    null_pct = \"N/A\"\n",
    "                    types = \"\"\n",
    "                    sample = \"\"\n",
    "            \n",
    "            results.append({\n",
    "                'Category': category,\n",
    "                'Field Name': field_name,\n",
    "                'Expected Path': field_path,\n",
    "                'Status': status,\n",
    "                'Null %': null_pct,\n",
    "                'Type(s)': types,\n",
    "                'Sample': sample\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run validation\n",
    "validation_df = validate_field_mappings(EXPECTED_FIELD_MAPPINGS, df_fields)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FIELD MAPPING VALIDATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal expected fields: {len(validation_df)}\")\n",
    "print(f\"Present: {len(validation_df[validation_df['Status'] == '‚úì Present'])}\")\n",
    "print(f\"Missing: {len(validation_df[validation_df['Status'] == '‚ùå Missing'])}\")\n",
    "print(f\"Similar: {len(validation_df[validation_df['Status'].str.contains('Similar', na=False)])}\")\n",
    "\n",
    "# Show missing fields\n",
    "missing = validation_df[validation_df['Status'] == '‚ùå Missing']\n",
    "if len(missing) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è MISSING FIELDS ({len(missing)}):\")\n",
    "    print(missing[['Category', 'Field Name', 'Expected Path']].to_string(index=False))\n",
    "\n",
    "# Show fields with high nulls\n",
    "high_null_expected = validation_df[\n",
    "    validation_df['Status'] == '‚úì Present'\n",
    "]\n",
    "if len(high_null_expected) > 0:\n",
    "    high_null_expected = high_null_expected[\n",
    "        high_null_expected['Null %'].str.rstrip('%').str.replace('N/A', '0').astype(float) > 50\n",
    "    ]\n",
    "    if len(high_null_expected) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è EXPECTED FIELDS WITH HIGH NULL RATES ({len(high_null_expected)}):\")\n",
    "        print(high_null_expected[['Category', 'Field Name', 'Null %']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "65321442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAFE FIELD ACCESSOR EXAMPLES\n",
      "================================================================================\n",
      "\n",
      "Use these patterns for null-safe field extraction:\n",
      "\n",
      "# data.general.matchId\n",
      "data.get('data', {}).get('general', {}).get('matchId', {}) or None\n",
      "\n",
      "# data.header.status.utcTime\n",
      "data.get('data', {}).get('header', {}).get('status', {}).get('utcTime', {}) or None\n",
      "\n",
      "# data.header.teams[0].name\n",
      "(data.get('data', {}).get('header', {}).get('teams', [])[0] if len(data.get('data', {}).get('header', {}).get('teams', [])) > 0 else None).get('name', {}) or None\n",
      "\n",
      "# data.content.shotmap.shots[0].expectedGoals\n",
      "(data.get('data', {}).get('content', {}).get('shotmap', {}).get('shots', [])[0] if len(data.get('data', {}).get('content', {}).get('shotmap', {}).get('shots', [])) > 0 else None).get('expectedGoals', {}) or None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate field access helper with null-safe navigation\n",
    "def generate_safe_field_accessor(field_path: str, default_value=\"None\") -> str:\n",
    "    \"\"\"Generate Python code for safe field access with null handling.\"\"\"\n",
    "    parts = field_path.split('.')\n",
    "    code = \"data\"\n",
    "    \n",
    "    for part in parts:\n",
    "        if '[' in part:  # Handle list access\n",
    "            base, idx = part.split('[')\n",
    "            idx = idx.rstrip(']')\n",
    "            code = f\"{code}.get('{base}', [])\"\n",
    "            code = f\"({code}[{idx}] if len({code}) > {idx} else None)\"\n",
    "        else:\n",
    "            code = f\"{code}.get('{part}', {{}})\"\n",
    "    \n",
    "    # Add final default value\n",
    "    return f\"{code} or {default_value}\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SAFE FIELD ACCESSOR EXAMPLES\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nUse these patterns for null-safe field extraction:\\n\")\n",
    "\n",
    "sample_fields = [\n",
    "    'data.general.matchId',\n",
    "    'data.header.status.utcTime',\n",
    "    'data.header.teams[0].name',\n",
    "    'data.content.shotmap.shots[0].expectedGoals'\n",
    "]\n",
    "\n",
    "for field in sample_fields:\n",
    "    code = generate_safe_field_accessor(field)\n",
    "    print(f\"# {field}\")\n",
    "    print(f\"{code}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "344cb8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "NAMING PATTERN ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Found 2565 fields with naming patterns to review:\n",
      "\n",
      "\n",
      "üìã camelCase_to_snake_case (1860 fields):\n",
      "                                                  Field Path Contains Suggestion\n",
      "                                 data.content.lineup.matchId  matchId   match_id\n",
      "                             data.content.matchFacts.matchId  matchId   match_id\n",
      "                                        data.general.matchId  matchId   match_id\n",
      "data.content.matchFacts.events.events[0].shotmapEvent.teamId   teamId    team_id\n",
      "data.content.matchFacts.events.events[1].shotmapEvent.teamId   teamId    team_id\n",
      "                  data.content.matchFacts.insights[0].teamId   teamId    team_id\n",
      "                  data.content.matchFacts.insights[1].teamId   teamId    team_id\n",
      "                  data.content.matchFacts.insights[2].teamId   teamId    team_id\n",
      "             data.content.matchFacts.playerOfTheMatch.teamId   teamId    team_id\n",
      " data.content.matchFacts.topPlayers.awayTopPlayers[0].teamId   teamId    team_id\n",
      "\n",
      "üìã inconsistent_abbreviations (371 fields):\n",
      "                                                                    Field Path Contains     Suggestion\n",
      "               data.content.matchFacts.playerOfTheMatch.stats[0].stats.xG + xA       xG expected_goals\n",
      "           data.content.matchFacts.playerOfTheMatch.stats[0].stats.xG + xA.key       xG expected_goals\n",
      "          data.content.matchFacts.playerOfTheMatch.stats[0].stats.xG + xA.stat       xG expected_goals\n",
      "     data.content.matchFacts.playerOfTheMatch.stats[0].stats.xG + xA.stat.type       xG expected_goals\n",
      "    data.content.matchFacts.playerOfTheMatch.stats[0].stats.xG + xA.stat.value       xG expected_goals\n",
      "           data.content.playerStats.1047489.stats[0].stats.Expected goals (xG)       xG expected_goals\n",
      "       data.content.playerStats.1047489.stats[0].stats.Expected goals (xG).key       xG expected_goals\n",
      "      data.content.playerStats.1047489.stats[0].stats.Expected goals (xG).stat       xG expected_goals\n",
      " data.content.playerStats.1047489.stats[0].stats.Expected goals (xG).stat.type       xG expected_goals\n",
      "data.content.playerStats.1047489.stats[0].stats.Expected goals (xG).stat.value       xG expected_goals\n",
      "\n",
      "üìã missing_prefixes (334 fields):\n",
      "                   Field Path          Contains                            Suggestion\n",
      "            data.header.teams data.header.teams Should check if it contains home/away\n",
      "         data.header.teams[0] data.header.teams Should check if it contains home/away\n",
      "data.header.teams[0].fifaRank data.header.teams Should check if it contains home/away\n",
      "      data.header.teams[0].id data.header.teams Should check if it contains home/away\n",
      "data.header.teams[0].imageUrl data.header.teams Should check if it contains home/away\n",
      "    data.header.teams[0].name data.header.teams Should check if it contains home/away\n",
      " data.header.teams[0].pageUrl data.header.teams Should check if it contains home/away\n",
      "   data.header.teams[0].score data.header.teams Should check if it contains home/away\n",
      "         data.header.teams[1] data.header.teams Should check if it contains home/away\n",
      "data.header.teams[1].fifaRank data.header.teams Should check if it contains home/away\n"
     ]
    }
   ],
   "source": [
    "# Detect naming mismatches and suggest corrections\n",
    "def find_naming_mismatches(actual_fields_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Find common naming patterns and mismatches.\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # Common naming issues\n",
    "    naming_patterns = {\n",
    "        'camelCase_to_snake_case': [\n",
    "            ('matchId', 'match_id'),\n",
    "            ('teamId', 'team_id'),\n",
    "            ('playerId', 'player_id'),\n",
    "            ('homeTeam', 'home_team'),\n",
    "            ('awayTeam', 'away_team'),\n",
    "            ('shotType', 'shot_type'),\n",
    "            ('expectedGoals', 'expected_goals'),\n",
    "        ],\n",
    "        'inconsistent_abbreviations': [\n",
    "            ('xG', 'expected_goals'),\n",
    "            ('xGOT', 'expected_goals_on_target'),\n",
    "            ('mins', 'minutes'),\n",
    "        ],\n",
    "        'missing_prefixes': [\n",
    "            ('data.header.teams', 'Should check if it contains home/away'),\n",
    "            ('data.content.stats.Periods', 'May need period prefix'),\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for pattern_type, patterns in naming_patterns.items():\n",
    "        for old_name, suggested_name in patterns:\n",
    "            # Find fields containing the old pattern\n",
    "            matches = actual_fields_df[\n",
    "                actual_fields_df['Field Path'].str.contains(old_name, case=True, na=False)\n",
    "            ]\n",
    "            \n",
    "            if len(matches) > 0:\n",
    "                for _, row in matches.iterrows():\n",
    "                    issues.append({\n",
    "                        'Pattern Type': pattern_type,\n",
    "                        'Field Path': row['Field Path'],\n",
    "                        'Contains': old_name,\n",
    "                        'Suggestion': suggested_name,\n",
    "                        'Current Type': row['Type(s)'],\n",
    "                        'Null %': row['Null %']\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(issues)\n",
    "\n",
    "naming_issues = find_naming_mismatches(df_fields)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"NAMING PATTERN ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if len(naming_issues) > 0:\n",
    "    print(f\"\\nFound {len(naming_issues)} fields with naming patterns to review:\\n\")\n",
    "    \n",
    "    # Group by pattern type\n",
    "    for pattern_type in naming_issues['Pattern Type'].unique():\n",
    "        subset = naming_issues[naming_issues['Pattern Type'] == pattern_type]\n",
    "        print(f\"\\nüìã {pattern_type} ({len(subset)} fields):\")\n",
    "        print(subset[['Field Path', 'Contains', 'Suggestion']].head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n‚úì No common naming pattern issues found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9710a640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "‚úì Generated: fotmob_validation_helper.py\n",
      "================================================================================\n",
      "\n",
      "This file contains:\n",
      "  - SafeFieldExtractor: For null-safe field access\n",
      "  - FieldValidator: For validating API responses\n",
      "\n",
      "You can import and use in your match processor!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create validation helper class\n",
    "validation_helper_code = '''\n",
    "\"\"\"\n",
    "FotMob Field Validation Helper\n",
    "Auto-generated based on actual API structure\n",
    "\"\"\"\n",
    "\n",
    "from typing import Any, Dict, Optional, List\n",
    "from pydantic import BaseModel, Field, field_validator, model_validator\n",
    "\n",
    "\n",
    "class SafeFieldExtractor:\n",
    "    \"\"\"Helper class for safe field extraction from FotMob API responses.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_get(data: Dict, path: str, default: Any = None) -> Any:\n",
    "        \"\"\"\n",
    "        Safely extract nested field from dictionary using dot notation.\n",
    "        \n",
    "        Args:\n",
    "            data: Source dictionary\n",
    "            path: Dot-separated path (e.g., 'general.matchId')\n",
    "            default: Default value if path not found\n",
    "        \n",
    "        Returns:\n",
    "            Value at path or default\n",
    "        \"\"\"\n",
    "        keys = path.split('.')\n",
    "        current = data\n",
    "        \n",
    "        for key in keys:\n",
    "            if isinstance(current, dict):\n",
    "                current = current.get(key)\n",
    "                if current is None:\n",
    "                    return default\n",
    "            elif isinstance(current, list) and key.isdigit():\n",
    "                idx = int(key)\n",
    "                current = current[idx] if idx < len(current) else None\n",
    "                if current is None:\n",
    "                    return default\n",
    "            else:\n",
    "                return default\n",
    "        \n",
    "        return current if current is not None else default\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_get_nested(data: Dict, *keys, default: Any = None) -> Any:\n",
    "        \"\"\"\n",
    "        Safely extract nested field using multiple keys.\n",
    "        \n",
    "        Example:\n",
    "            safe_get_nested(data, 'general', 'homeTeam', 'id', default=0)\n",
    "        \"\"\"\n",
    "        current = data\n",
    "        for key in keys:\n",
    "            if isinstance(current, dict):\n",
    "                current = current.get(key)\n",
    "                if current is None:\n",
    "                    return default\n",
    "            else:\n",
    "                return default\n",
    "        return current if current is not None else default\n",
    "\n",
    "\n",
    "class FieldValidator:\n",
    "    \"\"\"Validates FotMob API responses for required fields.\"\"\"\n",
    "    \n",
    "    REQUIRED_FIELDS = {\n",
    "        'general.matchId': int,\n",
    "        'general.homeTeam.id': int,\n",
    "        'general.awayTeam.id': int,\n",
    "        'header.status.finished': bool,\n",
    "    }\n",
    "    \n",
    "    OPTIONAL_FIELDS = {\n",
    "        'content.shotmap': dict,\n",
    "        'content.lineup': dict,\n",
    "        'content.playerStats': dict,\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def validate_response(cls, data: Dict[str, Any]) -> tuple[bool, List[str]]:\n",
    "        \"\"\"\n",
    "        Validate API response has required fields.\n",
    "        \n",
    "        Returns:\n",
    "            (is_valid, list_of_errors)\n",
    "        \"\"\"\n",
    "        errors = []\n",
    "        extractor = SafeFieldExtractor()\n",
    "        \n",
    "        for field_path, expected_type in cls.REQUIRED_FIELDS.items():\n",
    "            value = extractor.safe_get(data, field_path)\n",
    "            \n",
    "            if value is None:\n",
    "                errors.append(f\"Missing required field: {field_path}\")\n",
    "            elif not isinstance(value, expected_type):\n",
    "                errors.append(\n",
    "                    f\"Invalid type for {field_path}: \"\n",
    "                    f\"expected {expected_type.__name__}, got {type(value).__name__}\"\n",
    "                )\n",
    "        \n",
    "        return len(errors) == 0, errors\n",
    "    \n",
    "    @classmethod\n",
    "    def validate_and_report(cls, data: Dict[str, Any]) -> None:\n",
    "        \"\"\"Validate and print report.\"\"\"\n",
    "        is_valid, errors = cls.validate_response(data)\n",
    "        \n",
    "        if is_valid:\n",
    "            print(\"‚úì Validation passed: All required fields present\")\n",
    "        else:\n",
    "            print(f\"‚ùå Validation failed with {len(errors)} errors:\")\n",
    "            for error in errors:\n",
    "                print(f\"  - {error}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "from fotmob_validation_helper import SafeFieldExtractor, FieldValidator\n",
    "\n",
    "# Load match data\n",
    "with open('match_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Validate\n",
    "FieldValidator.validate_and_report(data)\n",
    "\n",
    "# Safe extraction\n",
    "extractor = SafeFieldExtractor()\n",
    "match_id = extractor.safe_get(data, 'general.matchId', default=0)\n",
    "home_team = extractor.safe_get_nested(data, 'general', 'homeTeam', 'name', default='Unknown')\n",
    "\"\"\"\n",
    "'''\n",
    "\n",
    "# Write to file\n",
    "with open('fotmob_validation_helper.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(validation_helper_code)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úì Generated: fotmob_validation_helper.py\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nThis file contains:\")\n",
    "print(\"  - SafeFieldExtractor: For null-safe field access\")\n",
    "print(\"  - FieldValidator: For validating API responses\")\n",
    "print(\"\\nYou can import and use in your match processor!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5eb19e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING VALIDATION HELPER ON ACTUAL DATA\n",
      "================================================================================\n",
      "\n",
      "Testing file 1: match_1000002056.json.gz\n",
      "  ‚ùå Invalid (1 errors):\n",
      "    - Invalid type for general.matchId: expected int, got str\n",
      "\n",
      "Testing file 2: match_1000003608.json.gz\n",
      "  ‚ùå Invalid (1 errors):\n",
      "    - Invalid type for general.matchId: expected int, got str\n",
      "\n",
      "Testing file 3: match_1000003609.json.gz\n",
      "  ‚ùå Invalid (1 errors):\n",
      "    - Invalid type for general.matchId: expected int, got str\n",
      "\n",
      "Testing file 4: match_4692335.json.gz\n",
      "  ‚ùå Invalid (1 errors):\n",
      "    - Invalid type for general.matchId: expected int, got str\n",
      "\n",
      "Testing file 5: match_4693111.json.gz\n",
      "  ‚ùå Invalid (1 errors):\n",
      "    - Invalid type for general.matchId: expected int, got str\n",
      "\n",
      "================================================================================\n",
      "VALIDATION SUMMARY\n",
      "================================================================================\n",
      "Files tested: 5\n",
      "Valid: 0\n",
      "Invalid: 5\n",
      "\n",
      "Most common errors:\n",
      "  5x: Invalid type for general.matchId: expected int, got str\n"
     ]
    }
   ],
   "source": [
    "# Test the validation helper on actual data\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTING VALIDATION HELPER ON ACTUAL DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load helper\n",
    "exec(validation_helper_code)\n",
    "\n",
    "# Test on sample files\n",
    "test_results = []\n",
    "for i, file_path in enumerate(all_files[:5], 1):\n",
    "    print(f\"\\nTesting file {i}: {file_path.name}\")\n",
    "    data = load_fotmob_json(file_path)\n",
    "    \n",
    "    # Extract data section\n",
    "    if 'data' in data:\n",
    "        test_data = data['data']\n",
    "    else:\n",
    "        test_data = data\n",
    "    \n",
    "    # Validate\n",
    "    is_valid, errors = FieldValidator.validate_response(test_data)\n",
    "    \n",
    "    test_results.append({\n",
    "        'file': file_path.name,\n",
    "        'valid': is_valid,\n",
    "        'errors': len(errors),\n",
    "        'error_list': errors\n",
    "    })\n",
    "    \n",
    "    if is_valid:\n",
    "        print(f\"  ‚úì Valid\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Invalid ({len(errors)} errors):\")\n",
    "        for error in errors[:3]:\n",
    "            print(f\"    - {error}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"Files tested: {len(test_results)}\")\n",
    "print(f\"Valid: {sum(1 for r in test_results if r['valid'])}\")\n",
    "print(f\"Invalid: {sum(1 for r in test_results if not r['valid'])}\")\n",
    "\n",
    "# Show common errors\n",
    "all_errors = []\n",
    "for result in test_results:\n",
    "    all_errors.extend(result['error_list'])\n",
    "\n",
    "if all_errors:\n",
    "    print(f\"\\nMost common errors:\")\n",
    "    from collections import Counter\n",
    "    error_counts = Counter(all_errors)\n",
    "    for error, count in error_counts.most_common(5):\n",
    "        print(f\"  {count}x: {error}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "46c42202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "‚úì Generated: match_processor_fixes.py\n",
      "================================================================================\n",
      "\n",
      "Preview of suggested fixes:\n",
      "\n",
      "# Suggested fixes for match_processor.py\n",
      "\n",
      "# Generated based on actual FotMob API structure\n",
      "\n",
      "\n",
      "\n",
      "# === SAFE EXTRACTION PATTERNS ===\n",
      "\n",
      "\n",
      "def safe_extract_nested(data: dict, *keys, default=None):\n",
      "    '''Safely extract nested dictionary values.'''\n",
      "    current = data\n",
      "    for key in keys:\n",
      "        if isinstance(current, dict):\n",
      "            current = current.get(key)\n",
      "            if current is None:\n",
      "                return default\n",
      "        else:\n",
      "            return default\n",
      "    return current if current is not None else default\n",
      "\n",
      "# Usage examples:\n",
      "# match_id = safe_extract_nested(response_data, 'general', 'matchId', default=None)\n",
      "# home_team = safe_extract_nested(response_data, 'general', 'homeTeam', 'name', default='Unknown')\n",
      "\n",
      "\n",
      "... (see full file for all suggestions)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate field mapping corrections for match processor\n",
    "def generate_processor_fixes(validation_df: pd.DataFrame, actual_fields_df: pd.DataFrame) -> str:\n",
    "    \"\"\"Generate code fixes for the match processor based on validation results.\"\"\"\n",
    "    \n",
    "    fixes = []\n",
    "    fixes.append(\"# Suggested fixes for match_processor.py\\n\")\n",
    "    fixes.append(\"# Generated based on actual FotMob API structure\\n\\n\")\n",
    "    \n",
    "    # Missing fields - suggest alternatives or defaults\n",
    "    missing = validation_df[validation_df['Status'] == '‚ùå Missing']\n",
    "    if len(missing) > 0:\n",
    "        fixes.append(\"# === MISSING FIELDS - ADD DEFAULT HANDLING ===\\n\")\n",
    "        for _, row in missing.iterrows():\n",
    "            field_name = row['Field Name']\n",
    "            expected_path = row['Expected Path']\n",
    "            \n",
    "            # Suggest fix\n",
    "            fixes.append(f\"# Missing: {expected_path}\")\n",
    "            fixes.append(f'# Add: {field_name} = response_data.get(\"{expected_path}\", None)\\n')\n",
    "    \n",
    "    # High null fields - suggest optional handling\n",
    "    high_null = validation_df[\n",
    "        validation_df['Status'] == '‚úì Present'\n",
    "    ]\n",
    "    \n",
    "    if len(high_null) > 0:\n",
    "        try:\n",
    "            high_null = high_null[\n",
    "                pd.to_numeric(high_null['Null %'].str.rstrip('%').str.replace('N/A', '0'), errors='coerce') > 50\n",
    "            ]\n",
    "            \n",
    "            if len(high_null) > 0:\n",
    "                fixes.append(\"\\n# === HIGH NULL RATE FIELDS - MAKE OPTIONAL ===\\n\")\n",
    "                for _, row in high_null.iterrows():\n",
    "                    field_name = row['Field Name']\n",
    "                    null_pct = row['Null %']\n",
    "                    fixes.append(f\"# {field_name}: {null_pct} null - consider Optional type\\n\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Generate safe extraction patterns\n",
    "    fixes.append(\"\\n# === SAFE EXTRACTION PATTERNS ===\\n\")\n",
    "    fixes.append(\"\"\"\n",
    "def safe_extract_nested(data: dict, *keys, default=None):\n",
    "    '''Safely extract nested dictionary values.'''\n",
    "    current = data\n",
    "    for key in keys:\n",
    "        if isinstance(current, dict):\n",
    "            current = current.get(key)\n",
    "            if current is None:\n",
    "                return default\n",
    "        else:\n",
    "            return default\n",
    "    return current if current is not None else default\n",
    "\n",
    "# Usage examples:\n",
    "# match_id = safe_extract_nested(response_data, 'general', 'matchId', default=None)\n",
    "# home_team = safe_extract_nested(response_data, 'general', 'homeTeam', 'name', default='Unknown')\n",
    "\"\"\")\n",
    "    \n",
    "    return '\\n'.join(fixes)\n",
    "\n",
    "# Generate fixes\n",
    "processor_fixes = generate_processor_fixes(validation_df, df_fields)\n",
    "\n",
    "# Save to file\n",
    "with open('match_processor_fixes.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(processor_fixes)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úì Generated: match_processor_fixes.py\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nPreview of suggested fixes:\\n\")\n",
    "print(processor_fixes[:1000])\n",
    "print(\"\\n... (see full file for all suggestions)\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9923effa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "‚úì COMPREHENSIVE REPORT GENERATED\n",
      "================================================================================\n",
      "\n",
      "Generated files:\n",
      "  1. fotmob_field_analysis.csv - All field details\n",
      "  2. fotmob_field_report.xlsx - Multi-sheet Excel report\n",
      "  3. fotmob_validation_helper.py - Python validation utilities\n",
      "  4. match_processor_fixes.py - Suggested code fixes\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF ISSUES FOUND\n",
      "================================================================================\n",
      "\n",
      "üìä Field Coverage:\n",
      "  Expected fields: 29\n",
      "  Present: 29 (100.0%)\n",
      "  Missing: 0 (0.0%)\n",
      "  Similar found: 0\n",
      "\n",
      "üìã Naming Issues:\n",
      "  Potential naming mismatches: 2565\n",
      "\n",
      "‚úÖ Validation Tests:\n",
      "  Files passed: 0/5\n",
      "  Files failed: 5/5\n",
      "\n",
      "================================================================================\n",
      "Next Steps:\n",
      "  1. Review fotmob_field_report.xlsx for detailed analysis\n",
      "  2. Apply fixes from match_processor_fixes.py\n",
      "  3. Use fotmob_validation_helper.py in your code\n",
      "  4. Run validation on all files before processing\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Export comprehensive field report\n",
    "comprehensive_report = {\n",
    "    'validation_results': validation_df,\n",
    "    'all_fields': df_fields,\n",
    "    'naming_issues': naming_issues if len(naming_issues) > 0 else pd.DataFrame(),\n",
    "    'test_results': pd.DataFrame(test_results) if test_results else pd.DataFrame()\n",
    "}\n",
    "\n",
    "# Save all reports to Excel\n",
    "with pd.ExcelWriter('fotmob_field_report.xlsx', engine='openpyxl') as writer:\n",
    "    comprehensive_report['validation_results'].to_excel(writer, sheet_name='Validation', index=False)\n",
    "    comprehensive_report['all_fields'].to_excel(writer, sheet_name='All Fields', index=False)\n",
    "    if len(comprehensive_report['naming_issues']) > 0:\n",
    "        comprehensive_report['naming_issues'].to_excel(writer, sheet_name='Naming Issues', index=False)\n",
    "    if len(comprehensive_report['test_results']) > 0:\n",
    "        comprehensive_report['test_results'].to_excel(writer, sheet_name='Test Results', index=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úì COMPREHENSIVE REPORT GENERATED\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. fotmob_field_analysis.csv - All field details\")\n",
    "print(\"  2. fotmob_field_report.xlsx - Multi-sheet Excel report\")\n",
    "print(\"  3. fotmob_validation_helper.py - Python validation utilities\")\n",
    "print(\"  4. match_processor_fixes.py - Suggested code fixes\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY OF ISSUES FOUND\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Summary statistics\n",
    "total_expected = len(validation_df)\n",
    "present = len(validation_df[validation_df['Status'] == '‚úì Present'])\n",
    "missing = len(validation_df[validation_df['Status'] == '‚ùå Missing'])\n",
    "similar = len(validation_df[validation_df['Status'].str.contains('Similar', na=False)])\n",
    "\n",
    "print(f\"\\nüìä Field Coverage:\")\n",
    "print(f\"  Expected fields: {total_expected}\")\n",
    "print(f\"  Present: {present} ({present/total_expected*100:.1f}%)\")\n",
    "print(f\"  Missing: {missing} ({missing/total_expected*100:.1f}%)\")\n",
    "print(f\"  Similar found: {similar}\")\n",
    "\n",
    "print(f\"\\nüìã Naming Issues:\")\n",
    "print(f\"  Potential naming mismatches: {len(naming_issues)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Validation Tests:\")\n",
    "if test_results:\n",
    "    valid_count = sum(1 for r in test_results if r['valid'])\n",
    "    print(f\"  Files passed: {valid_count}/{len(test_results)}\")\n",
    "    print(f\"  Files failed: {len(test_results) - valid_count}/{len(test_results)}\")\n",
    "else:\n",
    "    print(f\"  No tests run\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Next Steps:\")\n",
    "print(\"  1. Review fotmob_field_report.xlsx for detailed analysis\")\n",
    "print(\"  2. Apply fixes from match_processor_fixes.py\")\n",
    "print(\"  3. Use fotmob_validation_helper.py in your code\")\n",
    "print(\"  4. Run validation on all files before processing\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e160c485",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b8b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727716ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
